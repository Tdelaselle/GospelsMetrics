{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f782f76-c571-4c09-92b4-0e7b51cb9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Auguste 2024\n",
    "@author: ThÃ©otime de la Selle\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk as nl\n",
    "\n",
    "import cltk\n",
    "from cltk import NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea061d3b-fa72-431d-8cef-d6d226c6d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.path.abspath(cltk.__file__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92282078-8154-49cd-a6b4-271f6e8fc092",
   "metadata": {},
   "source": [
    "# Load and verify data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f96c3c-e1a6-4e09-9b49-a127dec1e653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Mark 5:14</td>\n",
       "      <td>â¸‚ÎšÎ±á½¶ Î¿á¼±â¸ƒ Î²ÏŒÏƒÎºÎ¿Î½Ï„ÎµÏ‚ â¸€Î±á½Ï„Î¿á½ºÏ‚ á¼”Ï†Ï…Î³Î¿Î½ ÎºÎ±á½¶ â¸€á¼€Ï€Î®Î³Î³ÎµÎ¹...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>Mark 15:4</td>\n",
       "      <td>á½ Î´á½² Î Î¹Î»á¾¶Ï„Î¿Ï‚ Ï€Î¬Î»Î¹Î½ â¸€á¼Ï€Î·ÏÏÏ„Î± Î±á½Ï„á½¸Î½ Î»Î­Î³Ï‰Î½Â· ÎŸá½Îº á¼€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Mark 8:8</td>\n",
       "      <td>â¸‚ÎºÎ±á½¶ á¼”Ï†Î±Î³Î¿Î½â¸ƒ ÎºÎ±á½¶ á¼Ï‡Î¿ÏÏ„Î¬ÏƒÎ¸Î·ÏƒÎ±Î½, ÎºÎ±á½¶ á¼¦ÏÎ±Î½ Ï€ÎµÏÎ¹ÏƒÏƒ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        verses                                               text\n",
       "162  Mark 5:14  â¸‚ÎšÎ±á½¶ Î¿á¼±â¸ƒ Î²ÏŒÏƒÎºÎ¿Î½Ï„ÎµÏ‚ â¸€Î±á½Ï„Î¿á½ºÏ‚ á¼”Ï†Ï…Î³Î¿Î½ ÎºÎ±á½¶ â¸€á¼€Ï€Î®Î³Î³ÎµÎ¹...\n",
       "610  Mark 15:4  á½ Î´á½² Î Î¹Î»á¾¶Ï„Î¿Ï‚ Ï€Î¬Î»Î¹Î½ â¸€á¼Ï€Î·ÏÏÏ„Î± Î±á½Ï„á½¸Î½ Î»Î­Î³Ï‰Î½Â· ÎŸá½Îº á¼€...\n",
       "291   Mark 8:8  â¸‚ÎºÎ±á½¶ á¼”Ï†Î±Î³Î¿Î½â¸ƒ ÎºÎ±á½¶ á¼Ï‡Î¿ÏÏ„Î¬ÏƒÎ¸Î·ÏƒÎ±Î½, ÎºÎ±á½¶ á¼¦ÏÎ±Î½ Ï€ÎµÏÎ¹ÏƒÏƒ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomber of verses : 673\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Matt 9:20</td>\n",
       "      <td>ÎšÎ±á½¶ á¼°Î´Î¿á½º Î³Ï…Î½á½´ Î±á¼±Î¼Î¿ÏÏÎ¿Î¿á¿¦ÏƒÎ± Î´ÏÎ´ÎµÎºÎ± á¼”Ï„Î· Ï€ÏÎ¿ÏƒÎµÎ»Î¸Î¿á¿¦...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Matt 8:22</td>\n",
       "      <td>á½ Î´á½² á¼¸Î·ÏƒÎ¿á¿¦Ï‚ â¸€Î»Î­Î³ÎµÎ¹ Î±á½Ï„á¿·Â· á¼ˆÎºÎ¿Î»Î¿ÏÎ¸ÎµÎ¹ Î¼Î¿Î¹, ÎºÎ±á½¶ á¼„Ï†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Matt 15:30</td>\n",
       "      <td>ÎºÎ±á½¶ Ï€ÏÎ¿Ïƒá¿†Î»Î¸Î¿Î½ Î±á½Ï„á¿· á½„Ï‡Î»Î¿Î¹ Ï€Î¿Î»Î»Î¿á½¶ á¼”Ï‡Î¿Î½Ï„ÎµÏ‚ Î¼ÎµÎ¸Ê¼ á¼‘...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         verses                                               text\n",
       "254   Matt 9:20  ÎšÎ±á½¶ á¼°Î´Î¿á½º Î³Ï…Î½á½´ Î±á¼±Î¼Î¿ÏÏÎ¿Î¿á¿¦ÏƒÎ± Î´ÏÎ´ÎµÎºÎ± á¼”Ï„Î· Ï€ÏÎ¿ÏƒÎµÎ»Î¸Î¿á¿¦...\n",
       "222   Matt 8:22  á½ Î´á½² á¼¸Î·ÏƒÎ¿á¿¦Ï‚ â¸€Î»Î­Î³ÎµÎ¹ Î±á½Ï„á¿·Â· á¼ˆÎºÎ¿Î»Î¿ÏÎ¸ÎµÎ¹ Î¼Î¿Î¹, ÎºÎ±á½¶ á¼„Ï†...\n",
       "518  Matt 15:30  ÎºÎ±á½¶ Ï€ÏÎ¿Ïƒá¿†Î»Î¸Î¿Î½ Î±á½Ï„á¿· á½„Ï‡Î»Î¿Î¹ Ï€Î¿Î»Î»Î¿á½¶ á¼”Ï‡Î¿Î½Ï„ÎµÏ‚ Î¼ÎµÎ¸Ê¼ á¼‘..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomber of verses : 1068\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Luke 6:17</td>\n",
       "      <td>ÎšÎ±á½¶ ÎºÎ±Ï„Î±Î²á½°Ï‚ Î¼ÎµÏ„Ê¼ Î±á½Ï„á¿¶Î½ á¼”ÏƒÏ„Î· á¼Ï€á½¶ Ï„ÏŒÏ€Î¿Ï… Ï€ÎµÎ´Î¹Î½Î¿á¿¦,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Luke 7:29</td>\n",
       "      <td>(ÎºÎ±á½¶ Ï€á¾¶Ï‚ á½ Î»Î±á½¸Ï‚ á¼€ÎºÎ¿ÏÏƒÎ±Ï‚ ÎºÎ±á½¶ Î¿á¼± Ï„ÎµÎ»á¿¶Î½Î±Î¹ á¼Î´Î¹ÎºÎ±Î¯Ï‰...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Luke 6:24</td>\n",
       "      <td>Ï€Î»á½´Î½ Î¿á½Î±á½¶ á½‘Î¼á¿–Î½ Ï„Î¿á¿–Ï‚ Ï€Î»Î¿Ï…ÏƒÎ¯Î¿Î¹Ï‚, á½…Ï„Î¹ á¼€Ï€Î­Ï‡ÎµÏ„Îµ Ï„á½´Î½...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        verses                                               text\n",
       "269  Luke 6:17  ÎšÎ±á½¶ ÎºÎ±Ï„Î±Î²á½°Ï‚ Î¼ÎµÏ„Ê¼ Î±á½Ï„á¿¶Î½ á¼”ÏƒÏ„Î· á¼Ï€á½¶ Ï„ÏŒÏ€Î¿Ï… Ï€ÎµÎ´Î¹Î½Î¿á¿¦,...\n",
       "330  Luke 7:29  (ÎºÎ±á½¶ Ï€á¾¶Ï‚ á½ Î»Î±á½¸Ï‚ á¼€ÎºÎ¿ÏÏƒÎ±Ï‚ ÎºÎ±á½¶ Î¿á¼± Ï„ÎµÎ»á¿¶Î½Î±Î¹ á¼Î´Î¹ÎºÎ±Î¯Ï‰...\n",
       "276  Luke 6:24  Ï€Î»á½´Î½ Î¿á½Î±á½¶ á½‘Î¼á¿–Î½ Ï„Î¿á¿–Ï‚ Ï€Î»Î¿Ï…ÏƒÎ¯Î¿Î¹Ï‚, á½…Ï„Î¹ á¼€Ï€Î­Ï‡ÎµÏ„Îµ Ï„á½´Î½..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomber of verses : 1149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>John 8:5</td>\n",
       "      <td>á¼Î½ Î´á½² Ï„á¿· Î½ÏŒÎ¼á¿³ ÎœÏ‰Ïƒá¿†Ï‚ á¼¡Î¼á¿–Î½ á¼Î½ÎµÏ„ÎµÎ¯Î»Î±Ï„Î¿ Ï„á½°Ï‚ Ï„Î¿Î¹Î±ÏÏ„...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>John 8:36</td>\n",
       "      <td>á¼á½°Î½ Î¿á½–Î½ á½ Ï…á¼±á½¸Ï‚ á½‘Î¼á¾¶Ï‚ á¼Î»ÎµÏ…Î¸ÎµÏÏÏƒá¿ƒ, á½„Î½Ï„Ï‰Ï‚ á¼Î»ÎµÏÎ¸ÎµÏÎ¿...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>John 10:38</td>\n",
       "      <td>Îµá¼° Î´á½² Ï€Î¿Î¹á¿¶, Îºá¼‚Î½ á¼Î¼Î¿á½¶ Î¼á½´ Ï€Î¹ÏƒÏ„ÎµÏÎ·Ï„Îµ Ï„Î¿á¿–Ï‚ á¼”ÏÎ³Î¿Î¹Ï‚ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         verses                                               text\n",
       "340    John 8:5  á¼Î½ Î´á½² Ï„á¿· Î½ÏŒÎ¼á¿³ ÎœÏ‰Ïƒá¿†Ï‚ á¼¡Î¼á¿–Î½ á¼Î½ÎµÏ„ÎµÎ¯Î»Î±Ï„Î¿ Ï„á½°Ï‚ Ï„Î¿Î¹Î±ÏÏ„...\n",
       "371   John 8:36  á¼á½°Î½ Î¿á½–Î½ á½ Ï…á¼±á½¸Ï‚ á½‘Î¼á¾¶Ï‚ á¼Î»ÎµÏ…Î¸ÎµÏÏÏƒá¿ƒ, á½„Î½Ï„Ï‰Ï‚ á¼Î»ÎµÏÎ¸ÎµÏÎ¿...\n",
       "473  John 10:38  Îµá¼° Î´á½² Ï€Î¿Î¹á¿¶, Îºá¼‚Î½ á¼Î¼Î¿á½¶ Î¼á½´ Ï€Î¹ÏƒÏ„ÎµÏÎ·Ï„Îµ Ï„Î¿á¿–Ï‚ á¼”ÏÎ³Î¿Î¹Ï‚ ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nomber of verses : 878\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Matt 1:1</td>\n",
       "      <td>Î’Î¯Î²Î»Î¿Ï‚ Î³ÎµÎ½Î­ÏƒÎµÏ‰Ï‚ á¼¸Î·ÏƒÎ¿á¿¦ Ï‡ÏÎ¹ÏƒÏ„Î¿á¿¦ Ï…á¼±Î¿á¿¦ Î”Î±Ï…á½¶Î´ Ï…á¼±Î¿á¿¦ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Matt 1:2</td>\n",
       "      <td>á¼ˆÎ²ÏÎ±á½°Î¼ á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ á¼¸ÏƒÎ±Î¬Îº, á¼¸ÏƒÎ±á½°Îº Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Matt 1:3</td>\n",
       "      <td>á¼¸Î¿ÏÎ´Î±Ï‚ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ Î¦Î±Ïá½²Ï‚ ÎºÎ±á½¶ Ï„á½¸Î½ Î–Î¬ÏÎ± á¼Îº ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Matt 1:4</td>\n",
       "      <td>á¼ˆÏá½°Î¼ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ á¼ˆÎ¼Î¹Î½Î±Î´Î¬Î², á¼ˆÎ¼Î¹Î½Î±Î´á½°Î² Î´á½² á¼Î³...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matt 1:5</td>\n",
       "      <td>Î£Î±Î»Î¼á½¼Î½ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ â¸‚Î’ÏŒÎµÏ‚ á¼Îº Ï„á¿†Ï‚ á¿¬Î±Ï‡Î¬Î², Î’ÏŒ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     verses                                               text\n",
       "0  Matt 1:1  Î’Î¯Î²Î»Î¿Ï‚ Î³ÎµÎ½Î­ÏƒÎµÏ‰Ï‚ á¼¸Î·ÏƒÎ¿á¿¦ Ï‡ÏÎ¹ÏƒÏ„Î¿á¿¦ Ï…á¼±Î¿á¿¦ Î”Î±Ï…á½¶Î´ Ï…á¼±Î¿á¿¦ ...\n",
       "1  Matt 1:2  á¼ˆÎ²ÏÎ±á½°Î¼ á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ á¼¸ÏƒÎ±Î¬Îº, á¼¸ÏƒÎ±á½°Îº Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½...\n",
       "2  Matt 1:3  á¼¸Î¿ÏÎ´Î±Ï‚ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ Î¦Î±Ïá½²Ï‚ ÎºÎ±á½¶ Ï„á½¸Î½ Î–Î¬ÏÎ± á¼Îº ...\n",
       "3  Matt 1:4  á¼ˆÏá½°Î¼ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ á¼ˆÎ¼Î¹Î½Î±Î´Î¬Î², á¼ˆÎ¼Î¹Î½Î±Î´á½°Î² Î´á½² á¼Î³...\n",
       "4  Matt 1:5  Î£Î±Î»Î¼á½¼Î½ Î´á½² á¼Î³Î­Î½Î½Î·ÏƒÎµÎ½ Ï„á½¸Î½ â¸‚Î’ÏŒÎµÏ‚ á¼Îº Ï„á¿†Ï‚ á¿¬Î±Ï‡Î¬Î², Î’ÏŒ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load (reading csv file) and format data\n",
    "def load_bible_texts(path):\n",
    "    df = pd.read_csv(path,delimiter='\\t', skiprows=(0),dtype=str,header=1)\n",
    "    df_rows = df.shape[0]\n",
    "    display(df.sample(3))\n",
    "    print(\"Nomber of verses :\",df_rows)\n",
    "    return df,df_rows\n",
    "\n",
    "Mark_accented,Mc_verses = load_bible_texts(\"data/Mark.txt\")\n",
    "Matt_accented,Mt_verses = load_bible_texts(\"data/Matt.txt\")\n",
    "Luke_accented,Lc_verses = load_bible_texts(\"data/Luke.txt\")\n",
    "John_accented,Jn_verses = load_bible_texts(\"data/John.txt\")\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "def strip_accents(s):\n",
    "   return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                  if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def unaccented_text(pd):\n",
    "    pd_unaccented = pd\n",
    "    for i in tqdm(range(pd.shape[0])):\n",
    "        pd_unaccented.text[i] = strip_accents(pd.text[i])\n",
    "    return pd_unaccented\n",
    "\n",
    "# Mark = unaccented_text(Mark_accented)\n",
    "# Matt = unaccented_text(Matt_accented)\n",
    "# Luke = unaccented_text(Luke_accented)\n",
    "# John = unaccented_text(John_accented)\n",
    "\n",
    "Mark = Mark_accented\n",
    "Matt = Matt_accented\n",
    "Luke = Luke_accented\n",
    "John = John_accented\n",
    "\n",
    "df_concat = [Matt,Mark,Luke,John]\n",
    "Evangiles = pd.concat(df_concat)\n",
    "Evangiles.reset_index(drop=True,inplace=True)\n",
    "Ev_verses = Evangiles.shape[0]\n",
    "display(Evangiles.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344c2e12-d714-4240-8818-41d50daa3c36",
   "metadata": {},
   "source": [
    "# Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd4e6742-c473-41fb-b663-0b98a27b99ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2783    á¼¤ÏÎ¾Î±Î½Ï„Î¿ Î´á½² ÎºÎ±Ï„Î·Î³Î¿ÏÎµá¿–Î½ Î±á½Ï„Î¿á¿¦ Î»Î­Î³Î¿Î½Ï„ÎµÏ‚ Ï„Î¿á¿¦Ï„Î¿Î½ Îµá½•...\n",
       "2625    ÎºÎ±á½¶ Î¿á½Ï‡ Îµá½•ÏÎ¹ÏƒÎºÎ¿Î½ Ï„á½¸ Ï„Î¯ Ï€Î¿Î¹Î®ÏƒÏ‰ÏƒÎ¹Î½ á½ Î»Î±á½¸Ï‚ Î³á½°Ï á¼…Ï€...\n",
       "1933    ÎºÎ±á½¶ Îµá¼¶Ï€ÎµÎ½ Ï€Ïá½¸Ï‚ Î±á½Ï„Î¿ÏÏ‚ Ï€Î¬Î½Ï„Ï‰Ï‚ á¼ÏÎµá¿–Ï„Î­ Î¼Î¿Î¹ Ï„á½´Î½ Ï€Î±...\n",
       "3658    á¼Î¾á¿†Î»Î¸ÎµÎ½ Î¿á½–Î½ á½ Ï€Î¹Î»á¾¶Ï„Î¿Ï‚ á¼”Î¾Ï‰ Ï€Ïá½¸Ï‚ Î±á½Ï„Î¿á½ºÏ‚ ÎºÎ±á½¶ Ï†Î·ÏƒÎ¯...\n",
       "3029    á¼€Ï†á¿†ÎºÎµÎ½ Î¿á½–Î½ Ï„á½´Î½ á½‘Î´ÏÎ¯Î±Î½ Î±á½Ï„á¿†Ï‚ á¼¡ Î³Ï…Î½á½´ ÎºÎ±á½¶ á¼€Ï€á¿†Î»Î¸ÎµÎ½...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize(df):\n",
    "    \n",
    "    # Normalize greek text and remove critical apparatus characters\n",
    "    from cltk.alphabet.grc import normalize_grc\n",
    "    from cltk.alphabet.grc import tonos_oxia_converter\n",
    "    from cltk.alphabet.grc import drop_critical_apparatus_char\n",
    "    from cltk.alphabet.grc import filter_non_greek\n",
    "    from cltk.alphabet.grc import expand_iota_subscript\n",
    "    df.text = [expand_iota_subscript(txt) for txt in df.text]\n",
    "    df.text = [drop_critical_apparatus_char(txt) for txt in df.text]\n",
    "    df.text = [filter_non_greek(txt) for txt in df.text]\n",
    "    # df.text = [tonos_oxia_converter(txt) for txt in df.text]\n",
    "    df.text = [normalize_grc(txt) for txt in df.text]\n",
    "\n",
    "    # Manually remove critical apparatus symbols\n",
    "    # crit_symbols_list = ['â¸€', 'â¸‚','â¸ƒ','âŸ§','âŸ¦'] # list of symbols\n",
    "    # df.text = df.text.replace(crit_symbols_list,'',regex=True)\n",
    "    \n",
    "    # Lower case\n",
    "    df.text = df.text.str.lower()\n",
    "    return df\n",
    "\n",
    "Mark = standardize(Mark)\n",
    "Matt = standardize(Matt)\n",
    "Luke = standardize(Luke)\n",
    "John = standardize(John)\n",
    "Evangiles = standardize(Evangiles)\n",
    "\n",
    "# Control\n",
    "Evangiles.text.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7449bbc-239d-45c5-8b63-7e67589e9797",
   "metadata": {},
   "source": [
    "# Cltk pipeline application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217c2fd4-aa83-40fd-aff8-5dc5184fd736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€ğ¤€ CLTK version '1.3.0'. When using the CLTK in research, please cite: https://aclanthology.org/2021.acl-demo.3/\n",
      "\n",
      "Pipeline for language 'Ancient Greek' (ISO: 'grc'): `GreekNormalizeProcess`, `GreekSpacyProcess`, `GreekEmbeddingsProcess`, `StopsProcess`.\n",
      "\n",
      "â¸– ``GreekSpacyProcess`` using OdyCy model by Center for Humanities Computing Aarhus from https://huggingface.co/chcaa . Please cite: https://aclanthology.org/2023.latechclfl-1.14\n",
      "â¸– ``LatinEmbeddingsProcess`` using word2vec model by University of Oslo from http://vectors.nlpl.eu/ . Please cite: https://aclanthology.org/W17-0237/\n",
      "\n",
      "â¸ To suppress these messages, instantiate ``NLP()`` with ``suppress_banner=True``.\n"
     ]
    }
   ],
   "source": [
    "cltk_nlp_grc = NLP(language=\"grc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbe9050b-885f-41ad-aa89-1cf77c1332a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mark: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:06<00:00, 101.83it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:05<00:00, 179.70it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:06<00:00, 167.26it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:05<00:00, 174.67it/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:22<00:00, 170.26it/s]\n"
     ]
    }
   ],
   "source": [
    "Mc_cltk_doc = [cltk_nlp_grc.analyze(text=Mark.text[i]) for i in tqdm(range(Mc_verses),desc=\"Mark\")]\n",
    "Mt_cltk_doc = [cltk_nlp_grc.analyze(text=Matt.text[i]) for i in tqdm(range(Mt_verses))]\n",
    "Lc_cltk_doc = [cltk_nlp_grc.analyze(text=Luke.text[i]) for i in tqdm(range(Lc_verses))]\n",
    "Jn_cltk_doc = [cltk_nlp_grc.analyze(text=John.text[i]) for i in tqdm(range(Jn_verses))]\n",
    "Ev_cltk_doc = [cltk_nlp_grc.analyze(text=Evangiles.text[i]) for i in tqdm(range(Ev_verses))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2df8ce-9246-4471-ab6c-6f97e04ddf07",
   "metadata": {},
   "source": [
    "# Dataframe pre-processing from Cltk doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e9294d-f88a-418f-86ac-f9b844367fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the additionnal stop words list (especially for lemmata)\n",
    "added_stop_words = ['Î´Îµá¿–','á½§Î´Îµ','á¼Î³Ï','á¼•Ï‰Ï‚','á¼€Î»Î»','á¼Î¬Î½','á¼•Î¾','ÎºÎ±Ï„Î¬','ÎºÎ±Î¯','Î±Ï…Ì“Ï„ÏŒÏ‚','Î¼ÎµÏ„Î¬','Î±á½Ï„á½¸Î½', 'Îµá½Î¸ÏÏ‚','ÏƒÏ', \"Ï„ÏŒÏ„Îµ\",\"Ï€á¾¶ÏƒÎ±\",\"Ï€á¾¶Ï‚\",\"á¼µÎ½Î±\",\"á½…Ï‚\",\"Ï„Î¯Ï‚\",\"Ï„Î¹Ï‚\",\"á¼€Ï€ÏŒ\",\"Î¼Î®\",'Ï„á¿¶Î¹','á½‘Ï€',\"Ï€á¿¶Ï‚\",\"á½…Ï„Î±Î½\",'á¼Ï€Î¯',\"Î´\",\"Îµá¼·Ï‚\",\"Î¿á½—Ï„Î¿Ï‚\",\"Ï€ÏÏŒÏ‚\",\"Ï€Ïá½¸Ï‚\",\"Ï€ÏÏŒ\",\"Î¿á½–Ï‚\",\"á½…Ï„Îµ\",\"Î³Î¬Ï\",\"Î´Î­\",\"Ï€á¾¶Ï‚\"]\n",
    "# + 'Îµá¼¶Î¼Î¹' + 'Îµá½–' + 'Ï€Î¿á¿¦' ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46c49ace-273c-4087-81d3-21546d4c31c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 246314.71it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 151160.25it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 105761.21it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 81890.53it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 402390.11it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 176478.06it/s]\n",
      "tfidf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:02<00:00, 233.58it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 215873.86it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 258684.62it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [00:00<00:00, 93320.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 250126.57it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 210701.63it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 97344.82it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 81175.66it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 440593.75it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 179403.13it/s]\n",
      "tfidf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:07<00:00, 135.89it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 179101.86it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 260149.64it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1068/1068 [00:00<00:00, 50248.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 241566.68it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 187410.28it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 106693.87it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 82653.12it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 439112.10it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 404571.47it/s]\n",
      "tfidf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:09<00:00, 127.61it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 160411.92it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 269910.69it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1149/1149 [00:00<00:00, 35173.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 121417.70it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 88424.11it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 60288.44it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 76988.67it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 409086.75it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 168694.41it/s]\n",
      "tfidf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:04<00:00, 181.19it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 230999.81it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 235912.81it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 878/878 [00:00<00:00, 62458.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- Processing of dataframe ---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokens: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 167009.80it/s]\n",
      "Lemmata: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 206247.63it/s]\n",
      "Tokens filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 160839.99it/s]\n",
      "Lemmata filtered: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 83978.35it/s]\n",
      "Bigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 455306.31it/s]\n",
      "Trigrams: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 379843.23it/s]\n",
      "tfidf: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [01:30<00:00, 41.64it/s]\n",
      "Part-of-Speech: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 203425.63it/s]\n",
      "Morphosyntactic features: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 243684.18it/s]\n",
      "Vocabulary: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3768/3768 [00:00<00:00, 97128.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>lemmata_tfidf</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>John 13:24</td>\n",
       "      <td>Î½ÎµÏÎµÎ¹ Î¿á½–Î½ Ï„Î¿ÏÏ„Ï‰Î¹ ÏƒÎ¯Î¼Ï‰Î½ Ï€Î­Ï„ÏÎ¿Ï‚ Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹ Ï„Î¯Ï‚ á¼‚Î½ ...</td>\n",
       "      <td>[Î½ÎµÏÎµÎ¹, Î¿á½–Î½, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹, ...</td>\n",
       "      <td>[Î½ÎµÏÏ‰, Î¿á½–Î½, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹,...</td>\n",
       "      <td>[Î½ÎµÏÎµÎ¹, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹, Ï„Î¯Ï‚, ...</td>\n",
       "      <td>[Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹, Îµá¼°Î¼Î¯, Î»Î­Î³Ï‰]</td>\n",
       "      <td>[(Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰), (ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚), (Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸...</td>\n",
       "      <td>[(Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚), (ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½...</td>\n",
       "      <td>[0.17431684772421588, 0.1636486204038483, 0.53...</td>\n",
       "      <td>[verb, adverb, adjective, adjective, noun, ver...</td>\n",
       "      <td>[[(admirative, conditional, desiderative, impe...</td>\n",
       "      <td>[Îµá¼°Î¼Î¯, Î»Î­Î³Ï‰, Î½ÎµÏÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹, ÏƒÎ¹Î¼ÏŒÏ‰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2665</th>\n",
       "      <td>Luke 20:40</td>\n",
       "      <td>Î¿á½ÎºÎ­Ï„Î¹ Î³á½°Ï á¼Ï„ÏŒÎ»Î¼Ï‰Î½ á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½ Î±á½Ï„á½¸Î½ Î¿á½Î´Î­Î½</td>\n",
       "      <td>[Î¿á½ÎºÎ­Ï„Î¹, Î³á½°Ï, á¼Ï„ÏŒÎ»Î¼Ï‰Î½, á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½, Î±á½Ï„á½¸Î½, Î¿á½Î´Î­Î½]</td>\n",
       "      <td>[Î¿á½ÎºÎ­Ï„Î¹, Î³Î¬Ï, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰, Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚]</td>\n",
       "      <td>[Î¿á½ÎºÎ­Ï„Î¹, á¼Ï„ÏŒÎ»Î¼Ï‰Î½, á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½, Î±á½Ï„á½¸Î½, Î¿á½Î´Î­Î½]</td>\n",
       "      <td>[Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰, Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚]</td>\n",
       "      <td>[(Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰), (Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰), (á¼Ï€ÎµÏÏ‰Ï„...</td>\n",
       "      <td>[(Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰), (Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰...</td>\n",
       "      <td>[0.13666993399265762, 0.4031742542355808, 0.43...</td>\n",
       "      <td>[adverb, adverb, verb, verb, pronoun, determiner]</td>\n",
       "      <td>[[(pos, neg)], [], [(habitual, imperfective, i...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚, Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Matt 25:39</td>\n",
       "      <td>Ï€ÏŒÏ„Îµ Î´Î­ ÏƒÎµ Îµá¼´Î´Î¿Î¼ÎµÎ½ á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î± á¼¢ á¼Î½ Ï†Ï…Î»Î±Îºá¿†Î¹ ÎºÎ±á½¶...</td>\n",
       "      <td>[Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÎµ, Îµá¼´Î´Î¿Î¼ÎµÎ½, á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î±, á¼¢, á¼Î½, Ï†Ï…Î»...</td>\n",
       "      <td>[Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÏ, á½ÏÎ¬Ï‰, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, á¼¤, á¼Î½, Ï†Ï…Î»Î±Îºá¿†Î¹, ...</td>\n",
       "      <td>[Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÎµ, Îµá¼´Î´Î¿Î¼ÎµÎ½, á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î±, Ï†Ï…Î»Î±Îºá¿†Î¹, á¼¤...</td>\n",
       "      <td>[Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…Î»Î±ÎºÎ®, á¼”ÏÏ‡Î¿Î¼Î±Î¹]</td>\n",
       "      <td>[(Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½), (Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰), (á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…...</td>\n",
       "      <td>[(Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰), (Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…Î»Î±...</td>\n",
       "      <td>[0.3165116656176926, 0.2630305175056568, 0.502...</td>\n",
       "      <td>[adverb, adverb, pronoun, verb, verb, coordina...</td>\n",
       "      <td>[[(article, contrastive, demonstrative, emphat...</td>\n",
       "      <td>[Îµá¼¶Î´Î¿Î½, Ï€Î¿Ï„Î­, Ï†Ï…Î»Î±ÎºÎ®, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, á¼”ÏÏ‡Î¿Î¼Î±Î¹]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Matt 4:19</td>\n",
       "      <td>ÎºÎ±á½¶ Î»Î­Î³ÎµÎ¹ Î±á½Ï„Î¿á¿–Ï‚ Î´Îµá¿¦Ï„Îµ á½€Ï€Î¯ÏƒÏ‰ Î¼Î¿Ï… ÎºÎ±á½¶ Ï€Î¿Î¹Î®ÏƒÏ‰ á½‘Î¼...</td>\n",
       "      <td>[ÎºÎ±á½¶, Î»Î­Î³ÎµÎ¹, Î±á½Ï„Î¿á¿–Ï‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Î¼Î¿Ï…, ÎºÎ±á½¶, Ï€...</td>\n",
       "      <td>[ÎºÎ±Î¯, Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, á¼Î³Ï, ÎºÎ±Î¯, Ï€Î¿Î¹...</td>\n",
       "      <td>[Î»Î­Î³ÎµÎ¹, Î±á½Ï„Î¿á¿–Ï‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Î¼Î¿Ï…, Ï€Î¿Î¹Î®ÏƒÏ‰, á½‘Î¼á¾¶...</td>\n",
       "      <td>[Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Ï€Î¿Î¹Î­Ï‰, á¼Î»Î¯Î¶Ï‰1, á¼„Î½Î¸...</td>\n",
       "      <td>[(Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ), (Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰)...</td>\n",
       "      <td>[(Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ), (Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰), ...</td>\n",
       "      <td>[0.12944004319112637, 0.1887926628999442, 0.27...</td>\n",
       "      <td>[coordinating_conjunction, verb, pronoun, inte...</td>\n",
       "      <td>[[], [(admirative, conditional, desiderative, ...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, Î»Î­Î³Ï‰, Ï€Î¿Î¹Î­Ï‰, á¼Î»Î¯Î¶Ï‰1, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2202</th>\n",
       "      <td>Luke 9:54</td>\n",
       "      <td>á¼°Î´ÏŒÎ½Ï„ÎµÏ‚ Î´á½² Î¿á¼± Î¼Î±Î¸Î·Ï„Î±á½¶ á¼°Î¬ÎºÏ‰Î²Î¿Ï‚ ÎºÎ±á½¶ á¼°Ï‰Î¬Î½Î½Î·Ï‚ Îµá¼¶Ï€Î±...</td>\n",
       "      <td>[á¼°Î´ÏŒÎ½Ï„ÎµÏ‚, Î´á½², Î¿á¼±, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, ÎºÎ±á½¶, á¼°Ï‰Î¬Î½Î½...</td>\n",
       "      <td>[á½ÏÎ¬Ï‰, Î´Î­, á½, Î¼Î±Î¸Î·Ï„Î®Ï‚, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, ÎºÎ±Î¯, á¼°Ï‰Î¬Î½(Î½)Î·Ï‚...</td>\n",
       "      <td>[á¼°Î´ÏŒÎ½Ï„ÎµÏ‚, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, á¼°Ï‰Î¬Î½Î½Î·Ï‚, Îµá¼¶Ï€Î±Î½, ÎºÏ...</td>\n",
       "      <td>[Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, Î¹á½¤Î±Î½Î½Î·Ï‚, Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹...</td>\n",
       "      <td>[(Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶), (Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚), (á¼°Î¬ÎºÏ‰Î²Î¿...</td>\n",
       "      <td>[(Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚), (Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚,...</td>\n",
       "      <td>[0.09295145692455532, 0.2511570339637543, 0.21...</td>\n",
       "      <td>[verb, adverb, determiner, noun, noun, coordin...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Îµá¼¶Î´Î¿Î½, Îµá¼¶Ï€Î¿Î½, Î¹á½¤Î±Î½Î½Î·Ï‚, ÎºÎ±Ï„Î±Î²Î±Î¯Î½Ï‰, ÎºÏÏÎ¹...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          verses                                               text  \\\n",
       "3498  John 13:24  Î½ÎµÏÎµÎ¹ Î¿á½–Î½ Ï„Î¿ÏÏ„Ï‰Î¹ ÏƒÎ¯Î¼Ï‰Î½ Ï€Î­Ï„ÏÎ¿Ï‚ Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹ Ï„Î¯Ï‚ á¼‚Î½ ...   \n",
       "2665  Luke 20:40            Î¿á½ÎºÎ­Ï„Î¹ Î³á½°Ï á¼Ï„ÏŒÎ»Î¼Ï‰Î½ á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½ Î±á½Ï„á½¸Î½ Î¿á½Î´Î­Î½   \n",
       "899   Matt 25:39  Ï€ÏŒÏ„Îµ Î´Î­ ÏƒÎµ Îµá¼´Î´Î¿Î¼ÎµÎ½ á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î± á¼¢ á¼Î½ Ï†Ï…Î»Î±Îºá¿†Î¹ ÎºÎ±á½¶...   \n",
       "83     Matt 4:19  ÎºÎ±á½¶ Î»Î­Î³ÎµÎ¹ Î±á½Ï„Î¿á¿–Ï‚ Î´Îµá¿¦Ï„Îµ á½€Ï€Î¯ÏƒÏ‰ Î¼Î¿Ï… ÎºÎ±á½¶ Ï€Î¿Î¹Î®ÏƒÏ‰ á½‘Î¼...   \n",
       "2202   Luke 9:54  á¼°Î´ÏŒÎ½Ï„ÎµÏ‚ Î´á½² Î¿á¼± Î¼Î±Î¸Î·Ï„Î±á½¶ á¼°Î¬ÎºÏ‰Î²Î¿Ï‚ ÎºÎ±á½¶ á¼°Ï‰Î¬Î½Î½Î·Ï‚ Îµá¼¶Ï€Î±...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "3498  [Î½ÎµÏÎµÎ¹, Î¿á½–Î½, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹, ...   \n",
       "2665     [Î¿á½ÎºÎ­Ï„Î¹, Î³á½°Ï, á¼Ï„ÏŒÎ»Î¼Ï‰Î½, á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½, Î±á½Ï„á½¸Î½, Î¿á½Î´Î­Î½]   \n",
       "899   [Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÎµ, Îµá¼´Î´Î¿Î¼ÎµÎ½, á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î±, á¼¢, á¼Î½, Ï†Ï…Î»...   \n",
       "83    [ÎºÎ±á½¶, Î»Î­Î³ÎµÎ¹, Î±á½Ï„Î¿á¿–Ï‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Î¼Î¿Ï…, ÎºÎ±á½¶, Ï€...   \n",
       "2202  [á¼°Î´ÏŒÎ½Ï„ÎµÏ‚, Î´á½², Î¿á¼±, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, ÎºÎ±á½¶, á¼°Ï‰Î¬Î½Î½...   \n",
       "\n",
       "                                                lemmata  \\\n",
       "3498  [Î½ÎµÏÏ‰, Î¿á½–Î½, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹,...   \n",
       "2665     [Î¿á½ÎºÎ­Ï„Î¹, Î³Î¬Ï, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰, Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚]   \n",
       "899   [Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÏ, á½ÏÎ¬Ï‰, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, á¼¤, á¼Î½, Ï†Ï…Î»Î±Îºá¿†Î¹, ...   \n",
       "83    [ÎºÎ±Î¯, Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, á¼Î³Ï, ÎºÎ±Î¯, Ï€Î¿Î¹...   \n",
       "2202  [á½ÏÎ¬Ï‰, Î´Î­, á½, Î¼Î±Î¸Î·Ï„Î®Ï‚, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, ÎºÎ±Î¯, á¼°Ï‰Î¬Î½(Î½)Î·Ï‚...   \n",
       "\n",
       "                                        tokens_filtered  \\\n",
       "3498  [Î½ÎµÏÎµÎ¹, Ï„Î¿ÏÏ„Ï‰Î¹, ÏƒÎ¯Î¼Ï‰Î½, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î¸Î­ÏƒÎ¸Î±Î¹, Ï„Î¯Ï‚, ...   \n",
       "2665          [Î¿á½ÎºÎ­Ï„Î¹, á¼Ï„ÏŒÎ»Î¼Ï‰Î½, á¼Ï€ÎµÏÏ‰Ï„á¾¶Î½, Î±á½Ï„á½¸Î½, Î¿á½Î´Î­Î½]   \n",
       "899   [Ï€ÏŒÏ„Îµ, Î´Î­, ÏƒÎµ, Îµá¼´Î´Î¿Î¼ÎµÎ½, á¼€ÏƒÎ¸ÎµÎ½Î¿á¿¦Î½Ï„Î±, Ï†Ï…Î»Î±Îºá¿†Î¹, á¼¤...   \n",
       "83    [Î»Î­Î³ÎµÎ¹, Î±á½Ï„Î¿á¿–Ï‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Î¼Î¿Ï…, Ï€Î¿Î¹Î®ÏƒÏ‰, á½‘Î¼á¾¶...   \n",
       "2202  [á¼°Î´ÏŒÎ½Ï„ÎµÏ‚, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, á¼°Ï‰Î¬Î½Î½Î·Ï‚, Îµá¼¶Ï€Î±Î½, ÎºÏ...   \n",
       "\n",
       "                                       lemmata_filtered  \\\n",
       "3498      [Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹, Îµá¼°Î¼Î¯, Î»Î­Î³Ï‰]   \n",
       "2665          [Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰, Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚]   \n",
       "899             [Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…Î»Î±ÎºÎ®, á¼”ÏÏ‡Î¿Î¼Î±Î¹]   \n",
       "83    [Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰, Ï€Î¿Î¹Î­Ï‰, á¼Î»Î¯Î¶Ï‰1, á¼„Î½Î¸...   \n",
       "2202  [Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚, Î¹á½¤Î±Î½Î½Î·Ï‚, Îµá¼¶Ï€Î¿Î½, ÎºÏÏÎ¹...   \n",
       "\n",
       "                                                bigrams  \\\n",
       "3498  [(Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰), (ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚), (Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸...   \n",
       "2665  [(Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰), (Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰), (á¼Ï€ÎµÏÏ‰Ï„...   \n",
       "899   [(Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½), (Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰), (á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…...   \n",
       "83    [(Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ), (Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰)...   \n",
       "2202  [(Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶), (Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚), (á¼°Î¬ÎºÏ‰Î²Î¿...   \n",
       "\n",
       "                                               trigrams  \\\n",
       "3498  [(Î½ÎµÏÏ‰, ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚), (ÏƒÎ¹Î¼ÏŒÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½...   \n",
       "2665  [(Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰), (Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰...   \n",
       "899   [(Ï€Î¿Ï„Î­, Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰), (Îµá¼¶Î´Î¿Î½, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, Ï†Ï…Î»Î±...   \n",
       "83    [(Î»Î­Î³Ï‰, Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ), (Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, á½€Ï€Î¯ÏƒÏ‰), ...   \n",
       "2202  [(Îµá¼¶Î´Î¿Î½, Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚), (Î¼Î±Î¸Î·Ï„Î±á½¶, á¼°Î¬ÎºÏ‰Î²Î¿Ï‚,...   \n",
       "\n",
       "                                          lemmata_tfidf  \\\n",
       "3498  [0.17431684772421588, 0.1636486204038483, 0.53...   \n",
       "2665  [0.13666993399265762, 0.4031742542355808, 0.43...   \n",
       "899   [0.3165116656176926, 0.2630305175056568, 0.502...   \n",
       "83    [0.12944004319112637, 0.1887926628999442, 0.27...   \n",
       "2202  [0.09295145692455532, 0.2511570339637543, 0.21...   \n",
       "\n",
       "                                                    pos  \\\n",
       "3498  [verb, adverb, adjective, adjective, noun, ver...   \n",
       "2665  [adverb, adverb, verb, verb, pronoun, determiner]   \n",
       "899   [adverb, adverb, pronoun, verb, verb, coordina...   \n",
       "83    [coordinating_conjunction, verb, pronoun, inte...   \n",
       "2202  [verb, adverb, determiner, noun, noun, coordin...   \n",
       "\n",
       "                                                 morpho  \\\n",
       "3498  [[(admirative, conditional, desiderative, impe...   \n",
       "2665  [[(pos, neg)], [], [(habitual, imperfective, i...   \n",
       "899   [[(article, contrastive, demonstrative, emphat...   \n",
       "83    [[], [(admirative, conditional, desiderative, ...   \n",
       "2202  [[(habitual, imperfective, iterative, perfecti...   \n",
       "\n",
       "                                             vocabulary  \n",
       "3498      [Îµá¼°Î¼Î¯, Î»Î­Î³Ï‰, Î½ÎµÏÏ‰, Ï€Î­Ï„ÏÎ¿Ï‚, Ï€Ï…Î½Î¸Î¬Î½Î¿Î¼Î±Î¹, ÏƒÎ¹Î¼ÏŒÏ‰]  \n",
       "2665          [Î±á½Ï„ÏŒÏ‚, Î¿á½Î´ÎµÎ¯Ï‚, Î¿á½ÎºÎ­Ï„Î¹, Ï„Î¿Î»Î¼Î¬Ï‰, á¼Ï€ÎµÏÏ‰Ï„Î¬Ï‰]  \n",
       "899             [Îµá¼¶Î´Î¿Î½, Ï€Î¿Ï„Î­, Ï†Ï…Î»Î±ÎºÎ®, á¼€ÏƒÎ¸ÎµÎ½Î­Ï‰, á¼”ÏÏ‡Î¿Î¼Î±Î¹]  \n",
       "83    [Î±á½Ï„ÏŒÏ‚, Î´Îµá¿¦Ï„Îµ, Î»Î­Î³Ï‰, Ï€Î¿Î¹Î­Ï‰, á¼Î»Î¯Î¶Ï‰1, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, ...  \n",
       "2202  [Î±á½Ï„ÏŒÏ‚, Îµá¼¶Î´Î¿Î½, Îµá¼¶Ï€Î¿Î½, Î¹á½¤Î±Î½Î½Î·Ï‚, ÎºÎ±Ï„Î±Î²Î±Î¯Î½Ï‰, ÎºÏÏÎ¹...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def text_dataframe_processing(df,doc):\n",
    "\n",
    "    print(\"-------- Processing of dataframe ---------\")  \n",
    "\n",
    "    # ----- Remove stop words and punctuation for adding filtered tokens and lemmata to dataframe\n",
    "    from cltk.stops.words import Stops\n",
    "    from cltk.text.processes import DefaultPunctuationRemovalProcess\n",
    "    from cltk.lemmatize.grc import GreekBackoffLemmatizer\n",
    "    \n",
    "    Punct_filter = DefaultPunctuationRemovalProcess(language='grc')\n",
    "    doc = [Punct_filter.run(d) for d in doc]\n",
    "    \n",
    "    # ----- Add tokens lemmata, and tokens_stops_filtered in dataframe\n",
    "    df['tokens'] = [d.tokens for d in tqdm(doc,desc=\"Tokens\")]\n",
    "    df['lemmata'] = [d.lemmata for d in tqdm(doc,desc=\"Lemmata\")]\n",
    "    df['tokens_filtered'] = [d.tokens_stops_filtered for d in tqdm(doc,desc=\"Tokens filtered\")]\n",
    "     \n",
    "    lemmatizer = GreekBackoffLemmatizer() # we have to lemmatize the filtered tokens\n",
    "    tokens_lemmata_filtered = [lemmatizer.lemmatize(lem) for lem in df.tokens_filtered]\n",
    "    lemmata_filtered = []\n",
    "    for lem in tokens_lemmata_filtered :    \n",
    "        lemmata_filtered.append([l[1] for l in lem])\n",
    "\n",
    "    # Removing stop words from lemmata (based on cltk.stops.words process but only through extra_stops additionnal list as remove_stopwords doesn't work for lemmata)\n",
    "    stops_obj = Stops(iso_code=\"grc\")\n",
    "    df['lemmata_filtered'] = [stops_obj.remove_stopwords(tokens=lem, extra_stops=added_stop_words) for lem in tqdm(lemmata_filtered,desc=\"Lemmata filtered\")]\n",
    "    \n",
    "    #----- Add n-grams\n",
    "    df['bigrams'] = [list(nl.bigrams(lem)) for lem in tqdm(df.lemmata_filtered,desc=\"Bigrams\")]\n",
    "    df['trigrams'] = [list(nl.trigrams(lem)) for lem in tqdm(df.lemmata_filtered,desc=\"Trigrams\")]\n",
    "\n",
    "    # ----- Add tf-idf score for each lemmata\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf = TfidfVectorizer(\n",
    "        analyzer='word',\n",
    "        tokenizer=lambda x: x,\n",
    "        preprocessor=lambda x: x,\n",
    "        token_pattern=None)  \n",
    "\n",
    "    # Learn vocabulary and idf, return document-term matrix. \n",
    "    doc_term_matrix = tfidf.fit_transform(df.lemmata_filtered)\n",
    "    tfidf_values = [doc_term_matrix[i,j] for i, j in zip(*doc_term_matrix.nonzero())]\n",
    "    id_tfidf = [i for i, j in zip(*doc_term_matrix.nonzero())]\n",
    "    tfidf_verses = []\n",
    "    for i in tqdm(range(df.shape[0]),desc=\"tfidf\"):\n",
    "        tfidf_verses.append([tfidf_values[index] for (index, item) in enumerate(id_tfidf) if item == i])\n",
    "    df['lemmata_tfidf'] = [tf for tf in tfidf_verses]\n",
    "\n",
    "    # ---- Add part-of-speech feature in dataframe\n",
    "    pos = []\n",
    "    for i in tqdm(range(df.shape[0]),desc=\"Part-of-Speech\"):\n",
    "        pos.append([str(word.pos) for word in doc[i].words])   \n",
    "    df['pos'] = pos\n",
    "\n",
    "    # ---- Add morphosyntactic features in dataframe\n",
    "    df['morpho'] = [d.morphosyntactic_features for d in tqdm(doc,desc=\"Morphosyntactic features\")]    \n",
    "\n",
    "    # ----- Add vocabulary feature in dataframe\n",
    "    from nltk.lm import Vocabulary\n",
    "    vocab = []\n",
    "    for i in tqdm(range(df.shape[0]),desc=\"Vocabulary\"):\n",
    "        vocab.append(sorted(Vocabulary(df.lemmata_filtered[i], unk_cutoff=1).counts))\n",
    "    \n",
    "    df['vocabulary'] = vocab\n",
    "    \n",
    "    return df\n",
    "\n",
    "Mark = text_dataframe_processing(Mark,Mc_cltk_doc)\n",
    "Matt = text_dataframe_processing(Matt,Mt_cltk_doc)\n",
    "Luke = text_dataframe_processing(Luke,Lc_cltk_doc)\n",
    "John = text_dataframe_processing(John,Jn_cltk_doc)\n",
    "Evangiles = text_dataframe_processing(Evangiles,Ev_cltk_doc)\n",
    "\n",
    "display(Evangiles.sample(5)) # To controle pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47791728-d542-4b4d-8302-d9c8e2b9049f",
   "metadata": {},
   "source": [
    "# Edit distance test on 2 verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9a1fb5-6649-4464-b16f-5b809be61a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>lemmata_tfidf</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mark 8:31</td>\n",
       "      <td>ÎºÎ±á½¶ á¼¤ÏÎ¾Î±Ï„Î¿ Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½ Î±á½Ï„Î¿á½ºÏ‚ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„...</td>\n",
       "      <td>[ÎºÎ±á½¶, á¼¤ÏÎ¾Î±Ï„Î¿, Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½, Î±á½Ï„Î¿á½ºÏ‚, á½…Ï„Î¹, Î´Îµá¿–, Ï„á½¸Î½...</td>\n",
       "      <td>[ÎºÎ±Î¯, á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, á½…Ï„Î¹, Î´Îµá¿–, á½, Ï…á¼±ÏŒÏ‚,...</td>\n",
       "      <td>[á¼¤ÏÎ¾Î±Ï„Î¿, Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½, Î±á½Ï„Î¿á½ºÏ‚, Î´Îµá¿–, Ï…á¼±á½¸Î½, á¼€Î½Î¸ÏÏÏ€Î¿...</td>\n",
       "      <td>[á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, ...</td>\n",
       "      <td>[(á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰), (Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, Ï…á¼±...</td>\n",
       "      <td>[(á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚), (Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, Ï…á¼±ÏŒÏ‚...</td>\n",
       "      <td>[0.19473592611178892, 0.08473834811934751, 0.2...</td>\n",
       "      <td>[coordinating_conjunction, verb, verb, pronoun...</td>\n",
       "      <td>[[], [(habitual, imperfective, iterative, perf...</td>\n",
       "      <td>[Î±á½Ï„ÏŒÏ‚, Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Ï€Î¬ÏƒÏ‡Ï‰, Ï€Î¿Î»ÏÏ‚, Ï€ÏÎ­...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      verses                                               text  \\\n",
       "0  Mark 8:31  ÎºÎ±á½¶ á¼¤ÏÎ¾Î±Ï„Î¿ Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½ Î±á½Ï„Î¿á½ºÏ‚ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [ÎºÎ±á½¶, á¼¤ÏÎ¾Î±Ï„Î¿, Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½, Î±á½Ï„Î¿á½ºÏ‚, á½…Ï„Î¹, Î´Îµá¿–, Ï„á½¸Î½...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [ÎºÎ±Î¯, á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, á½…Ï„Î¹, Î´Îµá¿–, á½, Ï…á¼±ÏŒÏ‚,...   \n",
       "\n",
       "                                     tokens_filtered  \\\n",
       "0  [á¼¤ÏÎ¾Î±Ï„Î¿, Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½, Î±á½Ï„Î¿á½ºÏ‚, Î´Îµá¿–, Ï…á¼±á½¸Î½, á¼€Î½Î¸ÏÏÏ€Î¿...   \n",
       "\n",
       "                                    lemmata_filtered  \\\n",
       "0  [á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, ...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰), (Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚), (Î±á½Ï„ÏŒÏ‚, Ï…á¼±...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [(á¼„ÏÏ‡Ï‰, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚), (Î´Î¹Î´Î¬ÏƒÎºÏ‰, Î±á½Ï„ÏŒÏ‚, Ï…á¼±ÏŒÏ‚...   \n",
       "\n",
       "                                       lemmata_tfidf  \\\n",
       "0  [0.19473592611178892, 0.08473834811934751, 0.2...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [coordinating_conjunction, verb, verb, pronoun...   \n",
       "\n",
       "                                              morpho  \\\n",
       "0  [[], [(habitual, imperfective, iterative, perf...   \n",
       "\n",
       "                                          vocabulary  \n",
       "0  [Î±á½Ï„ÏŒÏ‚, Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚, Î´Î¹Î´Î¬ÏƒÎºÏ‰, Ï€Î¬ÏƒÏ‡Ï‰, Ï€Î¿Î»ÏÏ‚, Ï€ÏÎ­...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verses</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmata</th>\n",
       "      <th>tokens_filtered</th>\n",
       "      <th>lemmata_filtered</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>lemmata_tfidf</th>\n",
       "      <th>pos</th>\n",
       "      <th>morpho</th>\n",
       "      <th>vocabulary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Luke 9:22</td>\n",
       "      <td>Îµá¼°Ï€á½¼Î½ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„Î¿á¿¦ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï€Î¿Î»Î»á½° Ï€Î±Î¸Îµ...</td>\n",
       "      <td>[Îµá¼°Ï€á½¼Î½, á½…Ï„Î¹, Î´Îµá¿–, Ï„á½¸Î½, Ï…á¼±á½¸Î½, Ï„Î¿á¿¦, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï€Î¿...</td>\n",
       "      <td>[Î»Î­Î³Ï‰, á½…Ï„Î¹, Î´Îµá¿–, á½, Ï…á¼±ÏŒÏ‚, á½, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, ...</td>\n",
       "      <td>[Îµá¼°Ï€á½¼Î½, Î´Îµá¿–, Ï…á¼±á½¸Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï€Î¿Î»Î»á½°, Ï€Î±Î¸Îµá¿–Î½, á¼€Ï€...</td>\n",
       "      <td>[Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, Ï€Î¬ÏƒÏ‡Ï‰, á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼...</td>\n",
       "      <td>[(Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚), (Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚), (á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€...</td>\n",
       "      <td>[(Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚), (Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»...</td>\n",
       "      <td>[0.20768776823374044, 0.21975362111698, 0.2688...</td>\n",
       "      <td>[verb, subordinating_conjunction, verb, determ...</td>\n",
       "      <td>[[(habitual, imperfective, iterative, perfecti...</td>\n",
       "      <td>[Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚, Îµá¼¶Ï€Î¿Î½, Ï€Î¬ÏƒÏ‡Ï‰, Ï€Î¿Î»ÏÏ‚, Ï€ÏÎ­ÏƒÎ²Ï…Ï‚, Ï„ÏÎ¯...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      verses                                               text  \\\n",
       "0  Luke 9:22  Îµá¼°Ï€á½¼Î½ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„Î¿á¿¦ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï€Î¿Î»Î»á½° Ï€Î±Î¸Îµ...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [Îµá¼°Ï€á½¼Î½, á½…Ï„Î¹, Î´Îµá¿–, Ï„á½¸Î½, Ï…á¼±á½¸Î½, Ï„Î¿á¿¦, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï€Î¿...   \n",
       "\n",
       "                                             lemmata  \\\n",
       "0  [Î»Î­Î³Ï‰, á½…Ï„Î¹, Î´Îµá¿–, á½, Ï…á¼±ÏŒÏ‚, á½, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, ...   \n",
       "\n",
       "                                     tokens_filtered  \\\n",
       "0  [Îµá¼°Ï€á½¼Î½, Î´Îµá¿–, Ï…á¼±á½¸Î½, á¼€Î½Î¸ÏÏÏ€Î¿Ï…, Ï€Î¿Î»Î»á½°, Ï€Î±Î¸Îµá¿–Î½, á¼€Ï€...   \n",
       "\n",
       "                                    lemmata_filtered  \\\n",
       "0  [Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»ÏÏ‚, Ï€Î¬ÏƒÏ‡Ï‰, á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚), (Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚), (á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€...   \n",
       "\n",
       "                                            trigrams  \\\n",
       "0  [(Îµá¼¶Ï€Î¿Î½, Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚), (Ï…á¼±ÏŒÏ‚, á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚, Ï€Î¿Î»...   \n",
       "\n",
       "                                       lemmata_tfidf  \\\n",
       "0  [0.20768776823374044, 0.21975362111698, 0.2688...   \n",
       "\n",
       "                                                 pos  \\\n",
       "0  [verb, subordinating_conjunction, verb, determ...   \n",
       "\n",
       "                                              morpho  \\\n",
       "0  [[(habitual, imperfective, iterative, perfecti...   \n",
       "\n",
       "                                          vocabulary  \n",
       "0  [Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚, Îµá¼¶Ï€Î¿Î½, Ï€Î¬ÏƒÏ‡Ï‰, Ï€Î¿Î»ÏÏ‚, Ï€ÏÎ­ÏƒÎ²Ï…Ï‚, Ï„ÏÎ¯...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "166\n",
      "\n",
      "----- text  -----\n",
      "\n",
      "ÎºÎ±á½¶ á¼¤ÏÎ¾Î±Ï„Î¿ Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½ Î±á½Ï„Î¿á½ºÏ‚ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„Î¿á¿¦ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï€Î¿Î»Î»á½° Ï€Î±Î¸Îµá¿–Î½ ÎºÎ±á½¶ á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹ á½‘Ï€á½¸ Ï„á¿¶Î½ Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½ ÎºÎ±á½¶ Ï„á¿¶Î½ á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½ ÎºÎ±á½¶ Ï„á¿¶Î½ Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½ ÎºÎ±á½¶ á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹ ÎºÎ±á½¶ Î¼ÎµÏ„á½° Ï„ÏÎµá¿–Ï‚ á¼¡Î¼Î­ÏÎ±Ï‚ á¼€Î½Î±ÏƒÏ„á¿†Î½Î±Î¹\n",
      "Îµá¼°Ï€á½¼Î½ á½…Ï„Î¹ Î´Îµá¿– Ï„á½¸Î½ Ï…á¼±á½¸Î½ Ï„Î¿á¿¦ á¼€Î½Î¸ÏÏÏ€Î¿Ï… Ï€Î¿Î»Î»á½° Ï€Î±Î¸Îµá¿–Î½ ÎºÎ±á½¶ á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹ á¼€Ï€á½¸ Ï„á¿¶Î½ Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½ ÎºÎ±á½¶ á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½ ÎºÎ±á½¶ Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½ ÎºÎ±á½¶ á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹ ÎºÎ±á½¶ Ï„á¿†Î¹ Ï„ÏÎ¯Ï„Î·Î¹ á¼¡Î¼Î­ÏÎ±Î¹ á¼Î³ÎµÏÎ¸á¿†Î½Î±Î¹\n",
      "Edit distance between 2 verses : 0.1144578313253012\n",
      "30\n",
      "25\n",
      "\n",
      "----- tokens  -----\n",
      "\n",
      "['ÎºÎ±á½¶', 'á¼¤ÏÎ¾Î±Ï„Î¿', 'Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½', 'Î±á½Ï„Î¿á½ºÏ‚', 'á½…Ï„Î¹', 'Î´Îµá¿–', 'Ï„á½¸Î½', 'Ï…á¼±á½¸Î½', 'Ï„Î¿á¿¦', 'á¼€Î½Î¸ÏÏÏ€Î¿Ï…', 'Ï€Î¿Î»Î»á½°', 'Ï€Î±Î¸Îµá¿–Î½', 'ÎºÎ±á½¶', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹', 'á½‘Ï€á½¸', 'Ï„á¿¶Î½', 'Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½', 'ÎºÎ±á½¶', 'Ï„á¿¶Î½', 'á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½', 'ÎºÎ±á½¶', 'Ï„á¿¶Î½', 'Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½', 'ÎºÎ±á½¶', 'á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹', 'ÎºÎ±á½¶', 'Î¼ÎµÏ„á½°', 'Ï„ÏÎµá¿–Ï‚', 'á¼¡Î¼Î­ÏÎ±Ï‚', 'á¼€Î½Î±ÏƒÏ„á¿†Î½Î±Î¹']\n",
      "['Îµá¼°Ï€á½¼Î½', 'á½…Ï„Î¹', 'Î´Îµá¿–', 'Ï„á½¸Î½', 'Ï…á¼±á½¸Î½', 'Ï„Î¿á¿¦', 'á¼€Î½Î¸ÏÏÏ€Î¿Ï…', 'Ï€Î¿Î»Î»á½°', 'Ï€Î±Î¸Îµá¿–Î½', 'ÎºÎ±á½¶', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹', 'á¼€Ï€á½¸', 'Ï„á¿¶Î½', 'Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½', 'ÎºÎ±á½¶', 'á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½', 'ÎºÎ±á½¶', 'Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½', 'ÎºÎ±á½¶', 'á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹', 'ÎºÎ±á½¶', 'Ï„á¿†Î¹', 'Ï„ÏÎ¯Ï„Î·Î¹', 'á¼¡Î¼Î­ÏÎ±Î¹', 'á¼Î³ÎµÏÎ¸á¿†Î½Î±Î¹']\n",
      "Edit distance between 2 verses : 0.24\n",
      "30\n",
      "25\n",
      "\n",
      "----- lemmata  -----\n",
      "\n",
      "['ÎºÎ±Î¯', 'á¼„ÏÏ‡Ï‰', 'Î´Î¹Î´Î¬ÏƒÎºÏ‰', 'Î±á½Ï„ÏŒÏ‚', 'á½…Ï„Î¹', 'Î´Îµá¿–', 'á½', 'Ï…á¼±ÏŒÏ‚', 'á½', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'Ï€Î¿Î»ÏÏ‚', 'Ï€Î¬ÏƒÏ‡Ï‰', 'ÎºÎ±Î¯', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'á½‘Ï€ÏŒ', 'á½', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'ÎºÎ±Î¯', 'á½', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'ÎºÎ±Î¯', 'á½', 'Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'ÎºÎ±Î¯', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'ÎºÎ±Î¯', 'Î¼ÎµÏ„Î¬', 'Ï„ÏÎµá¿–Ï‚', 'á¼¡Î¼Î­ÏÎ±', 'á¼€Î½Î¯ÏƒÏ„Î·Î¼Î¹']\n",
      "['Î»Î­Î³Ï‰', 'á½…Ï„Î¹', 'Î´Îµá¿–', 'á½', 'Ï…á¼±ÏŒÏ‚', 'á½', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'Ï€Î¿Î»ÏÏ‚', 'Ï€Î¬ÏƒÏ‡Ï‰', 'ÎºÎ±Î¯', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'á¼€Ï€ÏŒ', 'á½', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'ÎºÎ±Î¯', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'ÎºÎ±Î¯', 'Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'ÎºÎ±Î¯', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'ÎºÎ±Î¯', 'Ï„á¿†Î¹', 'Ï„ÏÎ¯Ï„Î·Î¹', 'á¼¡Î¼Î­ÏÎ±', 'á¼Î³ÎµÎ¯ÏÏ‰']\n",
      "Edit distance between 2 verses : 0.2\n",
      "16\n",
      "15\n",
      "\n",
      "----- tokens_filtered  -----\n",
      "\n",
      "['á¼¤ÏÎ¾Î±Ï„Î¿', 'Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹Î½', 'Î±á½Ï„Î¿á½ºÏ‚', 'Î´Îµá¿–', 'Ï…á¼±á½¸Î½', 'á¼€Î½Î¸ÏÏÏ€Î¿Ï…', 'Ï€Î¿Î»Î»á½°', 'Ï€Î±Î¸Îµá¿–Î½', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹', 'Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½', 'á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½', 'Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½', 'á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹', 'Ï„ÏÎµá¿–Ï‚', 'á¼¡Î¼Î­ÏÎ±Ï‚', 'á¼€Î½Î±ÏƒÏ„á¿†Î½Î±Î¹']\n",
      "['Îµá¼°Ï€á½¼Î½', 'Î´Îµá¿–', 'Ï…á¼±á½¸Î½', 'á¼€Î½Î¸ÏÏÏ€Î¿Ï…', 'Ï€Î¿Î»Î»á½°', 'Ï€Î±Î¸Îµá¿–Î½', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î±ÏƒÎ¸á¿†Î½Î±Î¹', 'Ï€ÏÎµÏƒÎ²Ï…Ï„Î­ÏÏ‰Î½', 'á¼€ÏÏ‡Î¹ÎµÏÎ­Ï‰Î½', 'Î³ÏÎ±Î¼Î¼Î±Ï„Î­Ï‰Î½', 'á¼€Ï€Î¿ÎºÏ„Î±Î½Î¸á¿†Î½Î±Î¹', 'Ï„á¿†Î¹', 'Ï„ÏÎ¯Ï„Î·Î¹', 'á¼¡Î¼Î­ÏÎ±Î¹', 'á¼Î³ÎµÏÎ¸á¿†Î½Î±Î¹']\n",
      "Edit distance between 2 verses : 0.4\n",
      "15\n",
      "13\n",
      "\n",
      "----- lemmata_filtered  -----\n",
      "\n",
      "['á¼„ÏÏ‡Ï‰', 'Î´Î¹Î´Î¬ÏƒÎºÏ‰', 'Î±á½Ï„ÏŒÏ‚', 'Ï…á¼±ÏŒÏ‚', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'Ï€Î¿Î»ÏÏ‚', 'Ï€Î¬ÏƒÏ‡Ï‰', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'Ï„ÏÎµá¿–Ï‚', 'á¼¡Î¼Î­ÏÎ±', 'á¼€Î½Î¯ÏƒÏ„Î·Î¼Î¹']\n",
      "['Îµá¼¶Ï€Î¿Î½', 'Ï…á¼±ÏŒÏ‚', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'Ï€Î¿Î»ÏÏ‚', 'Ï€Î¬ÏƒÏ‡Ï‰', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'Ï„ÏÎ¯Ï„Î¿Ï‚', 'á¼¡Î¼Î­ÏÎ±', 'á¼Î³ÎµÎ¯ÏÏ‰']\n",
      "Edit distance between 2 verses : 0.23076923076923078\n",
      "30\n",
      "25\n",
      "\n",
      "----- pos  -----\n",
      "\n",
      "['coordinating_conjunction', 'verb', 'verb', 'pronoun', 'subordinating_conjunction', 'verb', 'determiner', 'noun', 'determiner', 'noun', 'adjective', 'verb', 'coordinating_conjunction', 'verb', 'adposition', 'determiner', 'adjective', 'coordinating_conjunction', 'determiner', 'noun', 'coordinating_conjunction', 'determiner', 'noun', 'coordinating_conjunction', 'verb', 'coordinating_conjunction', 'adposition', 'numeral', 'noun', 'verb']\n",
      "['verb', 'subordinating_conjunction', 'verb', 'determiner', 'noun', 'determiner', 'noun', 'adjective', 'verb', 'coordinating_conjunction', 'verb', 'adposition', 'determiner', 'adjective', 'coordinating_conjunction', 'noun', 'coordinating_conjunction', 'noun', 'coordinating_conjunction', 'verb', 'coordinating_conjunction', 'determiner', 'noun', 'noun', 'verb']\n",
      "Edit distance between 2 verses : 0.08\n",
      "30\n",
      "25\n",
      "\n",
      "----- morpho  -----\n",
      "\n",
      "[{}, {Aspect: [perfective], Mood: [indicative], Number: [singular], Person: [third], Tense: [past], VerbForm: [finite], Voice: [middle]}, {Tense: [present], VerbForm: [infinitive], Voice: [active]}, {Case: [accusative], Gender: [masculine], Number: [plural], Person: [third], PronominalType: [personal]}, {}, {Mood: [indicative], Number: [singular], Person: [third], Tense: [present], VerbForm: [finite], Voice: [active]}, {Case: [accusative], Definiteness: [definite], Gender: [masculine], Number: [singular], PronominalType: [demonstrative]}, {Case: [accusative], Gender: [masculine], Number: [singular]}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [singular], PronominalType: [demonstrative]}, {Case: [genitive], Gender: [masculine], Number: [singular]}, {Case: [accusative], Degree: [positive], Gender: [neuter], Number: [plural]}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [active]}, {}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [passive]}, {}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [genitive], Degree: [comparative], Gender: [masculine], Number: [plural]}, {}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [genitive], Gender: [masculine], Number: [plural]}, {}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [genitive], Gender: [masculine], Number: [plural]}, {}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [passive]}, {}, {}, {Case: [accusative], Gender: [feminine], Number: [plural]}, {Case: [accusative], Gender: [feminine], Number: [plural]}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [active]}]\n",
      "[{Aspect: [perfective], Case: [nominative], Gender: [masculine], Number: [singular], Tense: [past], VerbForm: [participle], Voice: [active]}, {}, {Mood: [indicative], Number: [singular], Person: [third], Tense: [present], VerbForm: [finite], Voice: [active]}, {Case: [accusative], Definiteness: [definite], Gender: [masculine], Number: [singular], PronominalType: [demonstrative]}, {Case: [accusative], Gender: [masculine], Number: [singular]}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [singular], PronominalType: [demonstrative]}, {Case: [genitive], Gender: [masculine], Number: [singular]}, {Case: [accusative], Degree: [positive], Gender: [neuter], Number: [plural]}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [active]}, {}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [passive]}, {}, {Case: [genitive], Definiteness: [definite], Gender: [masculine], Number: [plural], PronominalType: [demonstrative]}, {Case: [genitive], Degree: [comparative], Gender: [masculine], Number: [plural]}, {}, {Case: [genitive], Gender: [masculine], Number: [plural]}, {}, {Case: [genitive], Gender: [masculine], Number: [plural]}, {}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [passive]}, {}, {Case: [genitive], Definiteness: [definite], Gender: [feminine], Number: [plural], PronominalType: [demonstrative]}, {Case: [nominative], Gender: [feminine], Number: [plural]}, {Case: [nominative], Gender: [feminine], Number: [plural]}, {Aspect: [perfective], Tense: [past], VerbForm: [infinitive], Voice: [passive]}]\n",
      "Edit distance between 2 verses : 1.0\n",
      "15\n",
      "13\n",
      "\n",
      "----- vocabulary  -----\n",
      "\n",
      "['Î±á½Ï„ÏŒÏ‚', 'Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'Î´Î¹Î´Î¬ÏƒÎºÏ‰', 'Ï€Î¬ÏƒÏ‡Ï‰', 'Ï€Î¿Î»ÏÏ‚', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'Ï„ÏÎµá¿–Ï‚', 'Ï…á¼±ÏŒÏ‚', 'á¼€Î½Î¯ÏƒÏ„Î·Î¼Î¹', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'á¼„ÏÏ‡Ï‰', 'á¼¡Î¼Î­ÏÎ±']\n",
      "['Î³ÏÎ±Î¼Î¼Î±Ï„ÎµÏÏ‚', 'Îµá¼¶Ï€Î¿Î½', 'Ï€Î¬ÏƒÏ‡Ï‰', 'Ï€Î¿Î»ÏÏ‚', 'Ï€ÏÎ­ÏƒÎ²Ï…Ï‚', 'Ï„ÏÎ¯Ï„Î¿Ï‚', 'Ï…á¼±ÏŒÏ‚', 'á¼€Ï€Î¿Î´Î¿ÎºÎ¹Î¼Î¬Î¶Ï‰', 'á¼€Ï€Î¿ÎºÏ„ÎµÎ¯Î½Ï‰', 'á¼€ÏÏ‡Î¹ÎµÏÎµÏÏ‚', 'á¼„Î½Î¸ÏÏ‰Ï€Î¿Ï‚', 'á¼Î³ÎµÎ¯ÏÏ‰', 'á¼¡Î¼Î­ÏÎ±']\n",
      "Edit distance between 2 verses : 0.23076923076923078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.23076923076923078"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edit distance test on verses\n",
    "id_verse_1 = \"Mark 8:31\"\n",
    "# id_verse_1 = \"Matt 16:21\"\n",
    "id_verse_2 = \"Luke 9:22\"\n",
    "\n",
    "def extract_verse(id_verse,df):   \n",
    "    extracted_verse = df[df.verses == id_verse].reset_index(drop=True)\n",
    "    display(extracted_verse)\n",
    "    return extracted_verse\n",
    "\n",
    "# Find the verses in dataframes\n",
    "v_1 = extract_verse(id_verse_1,Evangiles)\n",
    "v_2 = extract_verse(id_verse_2,Evangiles)\n",
    "\n",
    "def compute_distance(v_1,v_2,method,info=False,transpos=False):\n",
    "    str_v1 = v_1[method][0]\n",
    "    len_v1 = len(str_v1)\n",
    "    print(len_v1)\n",
    "    str_v2 = v_2[method][0]\n",
    "    len_v2 = len(str_v2)\n",
    "    print(len_v2)\n",
    "   \n",
    "    # Compute edit distance (characters)\n",
    "    edit_dist = (nl.edit_distance(str_v1, str_v2,transpositions=transpos)-abs(len_v1-len_v2))/min(len_v1,len_v2)  # Normalized distance 1\n",
    "    # edit_dist = (nl.edit_distance(str_v1, str_v2,transpositions=transpos))/np.max([len_v1,len_v2])  # Normalized distance  2  \n",
    "    # edit_dist = (nl.edit_distance(str_v1, str_v2)-abs(len_v1-len_v2))/min(len_v1,len_v2)  # Raw distance (unity : words/characters)\n",
    "\n",
    "    if info == True:\n",
    "        print(\"\\n-----\",method,\" -----\\n\")\n",
    "        print(str_v1)\n",
    "        print(str_v2)\n",
    "        print(f\"Edit distance between 2 verses :\",edit_dist)\n",
    "    return edit_dist\n",
    "\n",
    "compute_distance(v_1,v_2,'text',True)\n",
    "compute_distance(v_1,v_2,'tokens',True,True)\n",
    "compute_distance(v_1,v_2,'lemmata',True,True)\n",
    "compute_distance(v_1,v_2,'tokens_filtered',True,True)\n",
    "compute_distance(v_1,v_2,'lemmata_filtered',True,True)\n",
    "compute_distance(v_1,v_2,'pos',True,True)\n",
    "compute_distance(v_1,v_2,'morpho',True,True)\n",
    "compute_distance(v_1,v_2,'vocabulary',True,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd43c7f-1e88-4ce1-8c54-a9eee28d8991",
   "metadata": {},
   "source": [
    "# Save dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baa1f079-acc4-4565-841b-fd9b15da9458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/Evangiles.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Evangiles, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"data/Mark.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Mark, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"data/Matt.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Matt, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"data/Luke.pkl\", \"wb\") as file:\n",
    "    pickle.dump(Luke, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\"data/John.pkl\", \"wb\") as file:\n",
    "    pickle.dump(John, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7440751-faa1-4061-a78d-b61f94c47a2a",
   "metadata": {},
   "source": [
    "# Distance matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76704ad-74bc-464f-9b1d-aecb6a05f491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['á¼€ÏÏ‡á½´', 'Ï„Î¿á¿¦', 'Îµá½Î±Î³Î³ÎµÎ»Î¯Î¿Ï…', 'á¼°Î·ÏƒÎ¿á¿¦', 'Ï‡ÏÎ¹ÏƒÏ„Î¿á¿¦', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = Mark[Mark.index == 0].reset_index(drop=True)\n",
    "test['tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5995f226-50d2-4aa5-92b9-4fe8fede78b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 673/673 [01:52<00:00,  5.99it/s]\n"
     ]
    }
   ],
   "source": [
    "def distance_matrix(df,method):\n",
    "    len_df = df.shape[0]\n",
    "    dist_mat = np.ones((len_df,len_df))\n",
    "    for i in tqdm(range(len_df)):\n",
    "        for j in range(i,len_df):\n",
    "            v1 = df[df.index == i].reset_index(drop=True)\n",
    "            v2 = df[df.index == j].reset_index(drop=True)\n",
    "            dist_mat[i,j] = compute_distance(v1,v2,method)\n",
    "    return dist_mat\n",
    "\n",
    "# dist_mat = distance_matrix(Mark[Mark.index < 700],\"lemmata_filtered\")\n",
    "dist_mat_Mc_lemmata_filtered = distance_matrix(Mark,\"lemmata_filtered\")\n",
    "dist_mat_Mc_tokens_filtered = distance_matrix(Mark,\"tokens_filtered\")\n",
    "dist_mat_Mc_text = distance_matrix(Mark,\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1d8ea22-457b-4ac1-8c2e-2af40aec56b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Inconsistent shape between the condition and the input (got (704, 1) and (704,))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwidget\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[43msns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheatmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdist_mat\u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinewidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:446\u001b[0m, in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plot rectangular data as a color-encoded matrix.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03mThis is an Axes-level function and will draw the heatmap into the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    443\u001b[0m \n\u001b[1;32m    444\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;66;03m# Initialize the plotter object\u001b[39;00m\n\u001b[0;32m--> 446\u001b[0m plotter \u001b[38;5;241m=\u001b[39m \u001b[43m_HeatMapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobust\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mannot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mannot_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbar_kws\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxticklabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m                      \u001b[49m\u001b[43myticklabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Add the pcolormesh kwargs here\u001b[39;00m\n\u001b[1;32m    451\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlinewidths\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m linewidths\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/seaborn/matrix.py:115\u001b[0m, in \u001b[0;36m_HeatMapper.__init__\u001b[0;34m(self, data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, cbar, cbar_kws, xticklabels, yticklabels, mask)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Validate the mask and convert to DataFrame\u001b[39;00m\n\u001b[1;32m    113\u001b[0m mask \u001b[38;5;241m=\u001b[39m _matrix_mask(data, mask)\n\u001b[0;32m--> 115\u001b[0m plot_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_where\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# Get good names for the rows and columns\u001b[39;00m\n\u001b[1;32m    118\u001b[0m xtickevery \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py:1933\u001b[0m, in \u001b[0;36mmasked_where\u001b[0;34m(condition, a, copy)\u001b[0m\n\u001b[1;32m   1931\u001b[0m (cshape, ashape) \u001b[38;5;241m=\u001b[39m (cond\u001b[38;5;241m.\u001b[39mshape, a\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cshape \u001b[38;5;129;01mand\u001b[39;00m cshape \u001b[38;5;241m!=\u001b[39m ashape:\n\u001b[0;32m-> 1933\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInconsistent shape between the condition and the input\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1934\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (cshape, ashape))\n\u001b[1;32m   1935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(a, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_mask\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m   1936\u001b[0m     cond \u001b[38;5;241m=\u001b[39m mask_or(cond, a\u001b[38;5;241m.\u001b[39m_mask)\n",
      "\u001b[0;31mIndexError\u001b[0m: Inconsistent shape between the condition and the input (got (704, 1) and (704,))"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "ax = sns.heatmap(dist_mat[dist_mat<0.4], linewidth=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "30b9a7d7-5aa8-4bd5-95b2-a33a98f50735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['á¼”ÏÏ‡Î¿Î¼Î±Î¹', 'Î±á½Ï„ÏŒÏ‚', 'Î»ÎµÏ€Ïá½¸Ï‚', 'Ï€Î±ÏÎ±ÎºÎ±Î»Î­Ï‰', 'Î±á½Ï„ÏŒÏ‚', 'Î³Î¿Î½Ï…Ï€ÎµÏ„Î­Ï‰', 'Î»Î­Î³Ï‰', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î¸Î­Î»Ï‰', 'Î´ÏÎ½Î±Î¼Î±Î¹', 'á¼Î³Ï', 'ÎºÎ±Î¸Î±ÏÎ¯Î¶Ï‰']\n",
      "['á½€ÏÎ³Î¯Î¶Ï‰', 'á¼ÎºÏ„ÎµÎ¯Î½Ï‰', 'Ï‡ÎµÎ¯Ï', 'Î±á½Ï„ÏŒÏ‚', 'á¼…Ï€Ï„Ï‰', 'Î»Î­Î³Ï‰', 'Î±á½Ï„ÏŒÏ‚', 'á¼Î¸Î­Î»Ï‰', 'ÎºÎ±Î¸Î±ÏÎ¯Î¶Ï‰']\n"
     ]
    }
   ],
   "source": [
    "print(Mark.lemmata_filtered[39])\n",
    "print(Mark.lemmata_filtered[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915674f3-1993-4982-8355-1f7e4565b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"Distance_matrices/Distance_matrix\"+, \"wb\") as pickle_file:\n",
    "    pickle.dump(\n",
    "        reduced_model,\n",
    "        pickle_file,\n",
    "        protocol=pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
